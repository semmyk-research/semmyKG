OPENAI_API_KEY=your-openai-api-key

LLM_MODEL=your-LLM-model-Name 
## (in the format: provider/model-identifier)

OPENAI_API_BASE=your-LLM-inference-provider-endpoint 
## (for locally hosted llm inference server like LMStudio or Jan.ai, follow ollama host adding /v1: http://localhost:1234/v1)

OPENAI_API_EMBED_BASE=your-embedding-provider-endpoint 
## (for locally hosted, do not include /embedding)

LLM_MODEL_EMBED=your-embedding-model  
##(in the format: provider/embedding-name)

OLLAMA_HOST=http://localhost:11434
## change port #
OLLAMA_API_KEY=  ##(include if required)